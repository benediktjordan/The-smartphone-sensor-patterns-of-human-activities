#region import
import pandas as pd
import pickle
import time
import numpy as np

#endregion

#region create merge object/class
class Merge_and_Impute:
    def __init__(self, df_base, df_tomerge, timedelta):
        self.df_base = df_base
        self.df_tomerge = df_tomerge
        self.timedelta = timedelta
    def merge(df_base, df_tomerge, timedelta, columns_to_delete):
        df_final = pd.DataFrame()

        # harmonize column names
        ## if there is a column named "2" in df_tomerge, rename it to "device_id"
        if "2" in df_base.columns:
            df_base = df_base.rename(columns={"2": "device_id"})
        if "2" in df_tomerge.columns:
            df_tomerge = df_tomerge.rename(columns={"2": "device_id"})
        if "device_id" not in df_base.columns:
            for col in df_base.columns:
                if "device_id" in col:
                    df_base = df_base.rename(columns={col: "device_id"})
        if "device_id" not in df_tomerge.columns:
            for col in df_tomerge.columns:
                if "device_id" in col:
                    df_tomerge = df_tomerge.rename(columns={col: "device_id"})

        ## if there is a column named "sensor_timestamp", rename it to "timestamp"
        if "sensor_timestamp" in df_base.columns:
            df_base = df_base.rename(columns={"sensor_timestamp": "timestamp"})
        if "sensor_timestamp" in df_tomerge.columns:
            df_tomerge = df_tomerge.rename(columns={"sensor_timestamp": "timestamp"})

        user_count = 1
        total_users = len(df_base['device_id'].unique())
        # iterate through participants and ESM_timestamps
        for user in df_base['device_id'].unique():
            time_start = time.time()

            df_base_user = df_base[df_base['device_id'] == user]
            df_tomerge_user = df_tomerge[df_tomerge['device_id'] == user]

            # make sure that timestamp coluns are in datetime format
            df_base_user['timestamp'] = pd.to_datetime(df_base_user['timestamp'])
            df_tomerge_user['timestamp'] = pd.to_datetime(df_tomerge_user['timestamp'])

            # sort dataframes by timestamp
            df_base_user = df_base_user.sort_values(by='timestamp')
            df_tomerge_user = df_tomerge_user.sort_values(by='timestamp')

            # duplicate timestamp column for test purposes
            df_tomerge_user['timestamp_merged'] = df_tomerge_user['timestamp']

            # delete columns from df_tomerger_user that dont have to be merged
            df_tomerge_user = df_tomerge_user.drop(columns= columns_to_delete)

            # delete columns "Unnamed: 0", "0", "1" and "2" from df_sensor_user_event: all the information of these
            # columns is already contained in the JSON data
            #df_sensor_user_event = df_sensor_user_event.drop(columns=['Unnamed: 0', '0', '1', '2'])

            # merge dataframes
            df_merged = pd.merge_asof(df_base_user, df_tomerge_user, on='timestamp',
                                      tolerance=pd.Timedelta(timedelta))
            # Explanation: this function looks for every entry in the left timeseries if there is a
            # entry in the right timeseries which is max. "timedelta" time apart
            # TODO: include functionality so that also sensors with lesser frequency can be merged (i.e.
            #  locations, open_wheather etc.)

            # concatenate df_merged to df_final
            df_final = pd.concat([df_final, df_merged], axis=0)

            print("Time for User " + str(user_count) + "/" + str(total_users) + " was " + str((time.time()-time_start)/60) + " minutes")
            user_count += 1
        return df_final

    def impute_deleteNaN(df_final):
        # delete rows with NaN values
        df_final = df_final.dropna(axis=0)
        return df_final

    #TODO include here imputation methods:
    # - fill by last / next value
    # - fill by mean






# for human motion general: merge motion features & GPS features (speed & acceleration)
timedelta = "1000ms" # has to be in a format compatible with pd.TimeDelta()
columns_to_delete = ['device_id', 'label_human motion - general', "ESM_timestamp", "label_human motion - general"] # columns that should be deleted from df_tomerge
# load df_base: this should be the df with features with less "sampling rate"/frequency
# here: load motion features which are computed for 2 seconds
df_base = pd.read_csv("/Users/benediktjordan/Documents/MTS/Iteration01/Data/data_preparation/features/locations/locations-aroundevents_features-distance-speed-acceleration.csv")
# load df_tomerge: this should be the df with features with higher "sampling rate"/frequency
df_tomerge = pd.read_pickle("/Users/benediktjordan/Documents/MTS/Iteration01/Data/data_preparation/features/activity-label_human motion - general_highfrequencysensors-all_timeperiod-2 s_featureselection.pkl")

#test performance
#df_base = df_base.iloc[0:250000]
#df_tomerge = df_tomerge.iloc[0:100000]

# label df_tomerge with human motion general labels & delete all NaN rows
sensors_included = "all"
with open("/Users/benediktjordan/Documents/MTS/Iteration01/Data/data_preparation/labels/esm_" + sensors_included + "_transformed_labeled_dict.pkl",
        'rb') as f:
    dict_label = pickle.load(f)
df_base = labeling_sensor_df(df_base, dict_label, label_column_name = "label_human motion - general" , ESM_identifier_column = "ESM_timestamp")
# how many nans in "label_human motion - general" column?
print("There are so many NaN values in label column: " + str(df_base["label_human motion - general"].isna().sum()))
# delete all rows with NaN in "label_human motion - general" column
df_base = df_base.dropna(subset=["label_human motion - general"])

df_final = merge(df_base, df_tomerge, timedelta, columns_to_delete)
# save intermediate
#df_final.to_csv("/Users/benediktjordan/Downloads/df_final_intermediate.csv")
df_final.to_pickle("/Users/benediktjordan/Downloads/df_final_intermediate.pkl")


#temporary row
df_final = pd.read_csv("/Users/benediktjordan/Downloads/df_final_intermediate.csv")

# NaN imputation: delete all NaN values
print("There are so many NaN values in df_final before imputation: " + str(df_final[df_final.isna().any(axis=1)]))
#print rows containing nan values
df_final = impute_deleteNaN(df_final)
print("There are so many NaN values in df_final after imputation: " + str(df_final[df_final.isna().any(axis=1)]))

# delete unnecessary columns

# save df_final with pickle
df_final.to_pickle("/Users/benediktjordan/Documents/MTS/Iteration01/Data/data_preparation/features/activity-label_human motion - general_sensor-highfrequencysensorsAll(2seconds)-and-locations(1seconds).pkl")




# check results
## how many NaN values in df_final for each column?
print(df_final.isna().sum())







#testdata
df_base = pd.read_csv("/Users/benediktjordan/Documents/MTS/Iteration01/Data/data_preparation/features/locations/locations-aroundevents_features-distance-speed-acceleration.csv")
df_tomerge = pd.read_pickle("/Users/benediktjordan/Documents/MTS/Iteration01/Data/data_preparation/features/activity-label_human motion - general_highfrequencysensors-all_timeperiod-2 s_featureselection.pkl")

#use only 1000 rows for testing
df_base = df_base.iloc[0:1000]
df_tomerge = df_tomerge.iloc[0:1000]
#endregion

#testdata for timeseries merging
base_sensor = "accelerometer"
df_base_timeseries = pd.read_pickle("/Users/benediktjordan/Documents/MTS/Iteration01/Data/data_preparation/xmin_around_events/" + str(
    base_sensor) + "_esm_timeperiod_5 min.csv_JSONconverted.pkl")





# region data preparation: create merged timeseries
## import data
base_sensor = "linear_accelerometer"
sensors = [ "rotation"]

#sensors = ["accelerometer", "gravity",
#           "gyroscope", "magnetometer", "rotation"]



## merging function
def merge_unaligned_timeseries(df_base, df_tomerge, merge_sensor):
    df_final = pd.DataFrame()
    user_count = 0

    # iterate through devices and ESM_timestamps
    for user in df_base['2'].unique():
        time_a = time.time()
        print("Current user is: ", user)

        df_base_user = df_base[df_base['2'] == user]
        for event in df_base_user['ESM_timestamp'].unique():
            #print("Current event is: ", event)
            # get data for specific user and ESM event
            df_base_user_event = df_base_user[(df_base['2'] == user) & (df_base_user['ESM_timestamp'] == event)]
            df_sensor_user_event = df_tomerge[(df_tomerge['2'] == user) & (df_tomerge['ESM_timestamp'] == event)]

            # sort dataframes by timestamp
            df_base_user_event = df_base_user_event.sort_values(by='timestamp')
            df_sensor_user_event = df_sensor_user_event.sort_values(by='timestamp')

            # duplicate timestamp column for test purposes
            df_sensor_user_event['timestamp_' + str(merge_sensor)] = df_sensor_user_event['timestamp']

            # delete all ESM-related columns in df_sensor_user_event (otherwise they would be duplicated)
            df_sensor_user_event = df_sensor_user_event.drop(
                columns=['ESM_timestamp', "ESM_location", "ESM_location_time",
                         "ESM_bodyposition", "ESM_bodyposition_time",
                         "ESM_activity", "ESM_activity_time",
                         "ESM_smartphonelocation", "ESM_smartphonelocation_time",
                         "ESM_aligned", "ESM_aligned_time"])
            # delete columns "Unnamed: 0", "0", "1" and "2" from df_sensor_user_event: all the information of these
            # columns is already contained in the JSON data
            df_sensor_user_event = df_sensor_user_event.drop(columns=['Unnamed: 0', '0', '1', '2'])

            # merge dataframes
            df_merged = pd.merge_asof(df_base_user_event, df_sensor_user_event, on='timestamp',
                                      tolerance=pd.Timedelta("100ms"))
            # TODO: include functionality so that also sensors with lesser frequency can be merged (i.e.
            #  locations, open_wheather etc.)

            # add merged data to 00_general dataframe
            df_final = df_final.append(df_merged)

        time_b = time.time()
        print("User " + str(user_count) + "/" + str(len(df_base['2'].unique())))
        print("Time for user: ", time_b - time_a)
        user_count += 1

    return df_final



## iterate through sensors
df_base = pd.read_csv("/Users/benediktjordan/Documents/MTS/Iteration01/Data/" + str(
    base_sensor) + "_esm_timeperiod_5 min.csv_JSONconverted.csv",
                      parse_dates=['timestamp'], infer_datetime_format=True)

#region temporary
df_base = pd.read_csv(
    "/Users/benediktjordan/Documents/MTS/Iteration01/Data/magnetometer_esm_timeperiod_5 min_TimeseriesMerged.csv",
    parse_dates=['timestamp', "1", "timestamp_accelerometer"], infer_datetime_format=True)

test = df_base["1"] - df_base["timestamp"]
test2 = df_base["1"] - df_base["timestamp_accelerometer"]
test.describe()
test2.describe()

df_base = df_base.drop(columns=['ESM_timestamp_y', "ESM_location_y", "ESM_location_time_y",
                                "ESM_bodyposition_y", "ESM_bodyposition_time_y",
                                "ESM_activity_y", "ESM_activity_time_y",
                                "ESM_smartphonelocation_y", "ESM_smartphonelocation_time_y",
                                "ESM_aligned_y", "ESM_aligned_time_y",
                                "Unnamed: 0_y", "0_y", "1_y", "2_y", "3_y"])
df_base = df_base.rename(columns={"Unnamed: 0_x": "Unnamed: 0", "0_x": "0", "1_x": "1", "2_x": "2", "3_x": "3",
                                  "ESM_timestamp_x": "ESM_timestamp", "ESM_location_x": "ESM_location",
                                  "ESM_location_time_x": "ESM_location_time", "ESM_bodyposition_x": "ESM_bodyposition",
                                  "ESM_bodyposition_time_x": "ESM_bodyposition_time", "ESM_activity_x": "ESM_activity",
                                  "ESM_activity_time_x": "ESM_activity_time",
                                  "ESM_smartphonelocation_x": "ESM_smartphonelocation",
                                  "ESM_smartphonelocation_time_x": "ESM_smartphonelocation_time",
                                  "ESM_aligned_x": "ESM_aligned",
                                  "ESM_aligned_time_x": "ESM_aligned_time"})

# endregion temporary
#TODO: also merge sensors with lesser frequency (i.e. locations, open_weather etc.)

for sensor in sensors:
    time_begin = time.time()
    print("Current sensor is: ", sensor)
    df_sensor = pd.read_csv(
        "/Users/benediktjordan/Documents/MTS/Iteration01/Data/" + sensor + "_esm_timeperiod_5 min.csv_JSONconverted.csv",
        parse_dates=['timestamp'], infer_datetime_format=True)
    df_base = merge_unaligned_timeseries(df_base, df_tomerge=df_sensor, merge_sensor=sensor)
    df_base.to_csv("/Users/benediktjordan/Documents/MTS/Iteration01/Data/" + str(
        sensor) + "_esm_timeperiod_5 min_TimeseriesMerged.csv", index=False)
    time_end = time.time()
    print("Time for sensor ", sensor, " is: ", time_end - time_begin)

# save merged data
df_base.to_csv("/Users/benediktjordan/Documents/MTS/Iteration01/Data/esm_timeperiod_5 min_TimeseriesMerged.csv",
               index=False)

# endregion

#region visualize results of merging
# import merged dataframe
df_esm = pd.read_csv("/Users/benediktjordan/Documents/MTS/Iteration01/Data/esm_all_transformed_labeled.csv")
label_columns = [col for col in df_esm.columns if "label" in col]



#endregion